# Model Performans Değerlendirme Ölçütleri

## Giriş

Bir makine öğrenmesi modeli geliştirdiğimizde, temel sorumuz şudur: "Bu model ne kadar iyi çalışıyor?" Bu sorunun cevabını nesnel olarak verebilmek için performans ölçütlerine başvururuz. Bu ölçütler, modelimizin tahmin yeteneğini, hatalarını ve genel gücünü sayısal olarak ifade etmemizi sağlayan birer karne notu gibidir.

## Karışıklık Matrisi (Confusion Matrix)

Sınıflandırma problemlerinde, model performansını analiz etmeye genellikle **Karışıklık Matrisi (Confusion Matrix)** ile başlarız. Bu tablo, modelin tahminlerinin gerçek değerlerle karşılaştırmasını basit ve anlaşılır bir formatta sunar.

```
                      Tahmin Edilen
                 Pozitif (P)    Negatif (N)
              ┌──────────────┬──────────────┐
Gerçek    (P) │     TP       │      FN      │
              ├──────────────┼──────────────┤
          (N) │     FP       │      TN      │
              └──────────────┴──────────────┘
```

### Temel Tanımlar

*   **Gerçek Pozitif (True Positive - TP):** Modelin, pozitif bir durumu doğru bir şekilde pozitif olarak tahmin etmesi. (Örn: Hasta bir kişiye 'hasta' tanısı konulması.)
*   **Gerçek Negatif (True Negative - TN):** Modelin, negatif bir durumu doğru bir şekilde negatif olarak tahmin etmesi. (Örn: Sağlıklı bir kişiye 'sağlıklı' tanısı konulması.)
*   **Sahte Pozitif (False Positive - FP):** Modelin, negatif bir durumu hatalı bir şekilde pozitif olarak tahmin etmesi. Buna **Tip I Hata** da denir. (Örn: Sağlıklı bir kişiye 'hasta' tanısı konulması.)
*   **Sahte Negatif (False Negative - FN):** Modelin, pozitif bir durumu hatalı bir şekilde negatif olarak tahmin etmesi. Buna **Tip II Hata** da denir. (Örn: Hasta bir kişiye 'sağlıklı' tanısı konulması.)

### Örnek

1000 kişilik bir veri setinde modelimizin performansını değerlendirelim:
*   Gerçekte Pozitif (Hasta): 500 kişi
*   Gerçekte Negatif (Sağlıklı): 500 kişi

Modelimizin tahminleri sonucunda oluşan karışıklık matrisi:

```
              Tahmin
           P        N      Toplam
      ┌─────────┬─────────┬────────┐
  P   │   350   │   150   │   500  │
Gerçek├─────────┼─────────┼────────┤
  N   │   250   │   250   │   500  │
      └─────────┴─────────┴────────┘
Toplam   600       400      1000
```
Bu matris, aşağıda inceleyeceğimiz birçok performans ölçütünün temelini oluşturur.

## Sınıflandırma Performans Ölçütleri

Karışıklık matrisinden türetilen ve modelimizin yeteneklerini farklı açılardan değerlendirmemizi sağlayan temel metrikleri inceleyelim.

**1. Doğruluk (Accuracy)**

En temel ve anlaşılması en kolay ölçüttür. Basitçe, "Tüm tahminlerin yüzde kaçı doğru?" sorusunu yanıtlar.
```
Accuracy = (TP + TN) / (Toplam Veri Sayısı)
```
**Örneğimizde:** `(350 + 250) / 1000 = 0,60` (%60)

**Not:** Gençler, doğruluk metriği, özellikle sınıfların dengesiz dağıldığı (örneğin, 990 sağlıklı kişiye karşılık 10 hasta) veri setlerinde yanıltıcı olabilir. Düşünün ki bir model, herkese "sağlıklı" diyor. Bu model %99 doğruluk oranına sahip olabilir ama asıl aradığımız hasta kişileri bulma konusunda tamamen başarısızdır. İşte bu yüzden daha incelikli metriklere ihtiyaç duyarız.

**2. Kesinlik (Precision)**

Bu metrik, modelin pozitif tahminlerinin kalitesine odaklanır: "'Pozitif' olarak etiketlediklerimizin ne kadarı gerçekten pozitifti?"
```
Precision = TP / (TP + FP)
```
**Örneğimizde:** `350 / (350 + 250) = 350 / 600 ≈ 0,58`

**Yorumlama:** Yüksek kesinlik, modelin bir örneğe "pozitif" dediğinde buna büyük ölçüde güvenebileceğimiz anlamına gelir. Sahte pozitiflerin (FP) maliyetinin yüksek olduğu durumlarda kritik bir metriktir. Örneğin, bir e-postanın yanlışlıkla spam olarak işaretlenmesi, önemli bir iletişimin kaçırılmasına neden olabilir. Bu senaryoda yüksek kesinlik hedefleriz.

**3. Duyarlılık (Recall / Sensitivity)**

Duyarlılık, pozitif sınıfı ne kadar iyi "yakalayabildiğimizi" ölçer: "Gerçekte pozitif olan vakaların yüzde kaçını tespit edebildik?"
```
Recall = TP / (TP + FN)
```
**Örneğimizde:** `350 / (350 + 150) = 350 / 500 = 0,70`

**Yorumlama:** Yüksek duyarlılık, modelin pozitif vakaları atlamadığını gösterir. Sahte negatiflerin (FN) maliyetinin yüksek olduğu durumlarda hayati önem taşır. Örneğin, ciddi bir hastalığın teşhis edilememesi, bir hastanın tedavi şansını kaybetmesine yol açabilir. Bu durumda duyarlılığı maksimize etmeye çalışırız.

**4. F1-Skoru (F1-Score)**

Genellikle kesinlik ve duyarlılık arasında bir denge kurmamız gerekir. Biri artarken diğeri azalma eğilimindedir. F1-Skoru, bu iki metriğin harmonik ortalamasını alarak bu dengeyi tek bir sayıda özetler. Her iki metriğin de önemli olduğu durumlarda kullanılır.
```
F1-Score = (2 × Precision × Recall) / (Precision + Recall)
```
**Örneğimizde:** `(2 × 0,58 × 0,70) / (0,58 + 0,70) ≈ 0,63`

**Yorumlama:** F1-Skoru, modelin hem sahte pozitiflerden kaçınma (Precision) hem de gerçek pozitifleri yakalama (Recall) yeteneklerini dengeli bir şekilde ölçer. Özellikle dengesiz veri setlerinde doğruluk (accuracy) metriğine göre çok daha güvenilir bir performans göstergesidir.

**5. Kappa Katsayısı (Cohen's Kappa)**

Doğruluk (Accuracy) metriği, özellikle sınıfların dengesiz dağıldığı durumlarda yanıltıcı olabileceğini konuşmuştuk. İşte bu noktada Kappa katsayısı devreye girer ve bize daha incelikli bir bakış açısı sunar. Kappa, modelimizin performansını, tamamen rastgele tahmin yapan bir modelin performansıyla karşılaştırır. Yani, "Modelimizin başarısı, şans faktörünün ne kadar ötesinde?" sorusuna cevap arar.

Modelin doğruluğunun, sadece sınıf dağılımlarına bakarak rastgele tahmin yapıldığında elde edilecek doğruluktan ne kadar daha iyi olduğunu gösterir. Bu katsayı genellikle -1 ile +1 arasında bir değer alır ve bu değerin yorumlanması için genel kabul görmüş bir ölçek bulunur.

| Kappa Değeri      | Yorumlama (Uyum Düzeyi) |
| ----------------- | ----------------------- |
| < 0               | Uyum Yok                |
| 0.00 – 0.20       | Çok Zayıf               |
| 0.21 – 0.40       | Zayıf                   |
| 0.41 – 0.60       | Orta                    |
| 0.61 – 0.80       | İyi                     |
| 0.81 – 1.00       | Çok İyi / Mükemmel      |

Peki bu değer nasıl hesaplanır? Gençler, formülün arkasındaki mantık oldukça sezgiseldir:

`Kappa = (Po - Pe) / (1 - Pe)`

*   **`Po` (Observed Agreement):** Bu, modelimizin gözlemlenen doğruluğudur. Yani bildiğimiz standart **Accuracy** metriğidir.
*   **`Pe` (Expected Agreement):** Bu ise "şans eseri beklenen uyum"dur. Modelin ve gerçek etiketlerin, tamamen tesadüfen aynı fikirde olma olasılığını ifade eder. Her sınıfın gerçek ve tahmin edilen oranları dikkate alınarak hesaplanır.

Formül, modelimizin gerçek doğruluğundan (`Po`) şans eseri elde edilecek doğruluğu (`Pe`) çıkarır ve bu farkı, şansın üzerinde elde edilebilecek maksimum başarıya (`1 - Pe`) oranlar. Bu sayede Kappa, modelin sadece doğru tahmin yapma oranını değil, aynı zamanda bu doğruluğun ne kadarının tesadüfi olmadığını da hesaba katar. Bu özelliği, onu özellikle tıp veya finans gibi alanlarda dengesiz veri setleri üzerinde çalışırken doğruluk metriğine göre çok daha güvenilir bir alternatif yapar.

**6. Ağırlıklı Ortalama (Weighted Average)**

Şimdi, birden fazla sınıfımız olduğunda (örneğimizdeki gibi ikili sınıflandırmanın ötesinde) metrikleri nasıl genelleyeceğimizi düşünelim. Örneğin, üç sınıfımız var: A, B ve C. Her sınıf için ayrı ayrı Precision, Recall ve F1-Skoru hesaplayabiliriz. Peki modelin genel performansı nedir?

Burada devreye ortalama alma yöntemleri giriyor. En yaygın olanlardan biri **ağırlıklı ortalamadır**.

Bu yaklaşım, her sınıfın metrik skorunu hesaplarken o sınıfın veri setindeki "ağırlığını" yani örnek sayısını (destek/support) dikkate alır. Sınıflar dengesizse bu çok önemlidir. Örneğin, 1000 örneklik bir veri setinde 800 tane A sınıfı, 150 tane B sınıfı ve 50 tane C sınıfı varsa, A sınıfının performansı genel skoru daha fazla etkilemelidir.

**Hesaplanışı:**
Her sınıf için metrik (örneğin F1-Skoru) hesaplanır.
`Ağırlıklı F1 = (F1_A * 800 + F1_B * 150 + F1_C * 50) / (800 + 150 + 50)`

Bu yöntem, her bir örneğin eşit derecede önemli olduğunu varsayar ve modelin genel performansını, veri setinin yapısını yansıtacak şekilde adil bir biçimde özetler. Scikit-learn gibi kütüphanelerin sınıflandırma raporlarında bu değeri "weighted avg" olarak görürsünüz.

## ROC Eğrisinin Kökeni ve Oluşturulması

ROC eğrisinin mantığını anlamak için kökenine inmek faydalı olacaktır. Bu kavram, II. Dünya Savaşı sırasında radar operatörlerinin sinyalleri yorumlama performansını ölçmek için geliştirilmiştir. Operatörün görevi, radar ekranındaki sinyallerin bir düşman uçağı mı (pozitif durum) yoksa zararsız bir kuş sürüsü veya atmosferik bir gürültü mü (negatif durum) olduğuna karar vermektir.

Operatörün karar verme "hassasiyeti" kritik bir rol oynar.
*   **Çok hassas olursa:** En zayıf sinyali bile düşman olarak işaretler. Bu durumda hiçbir düşmanı kaçırmaz (yüksek **Gerçek Pozitif Oranı**), ancak çok sayıda yanlış alarm verir (yüksek **Sahte Pozitif Oranı**).
*   **Az hassas olursa:** Sadece çok güçlü ve net sinyalleri düşman olarak işaretler. Bu durumda yanlış alarm sayısı çok az olur (düşük **Sahte Pozitif Oranı**), ancak bazı gerçek düşmanları gözden kaçırabilir (düşük **Gerçek Pozitif Oranı**).

ROC eğrisi, operatörün (veya makine öğrenmesi modelimizin) bu hassasiyet seviyesinin tüm olası değerleri için doğru tespitler ile yanlış alarmlar arasındaki dengeyi görselleştiren bir grafiktir.

Şimdi gençler, bu eğrinin nasıl ortaya çıktığını somut bir örnekle anlayalım.

Hayali bir güvenlik sistemimiz olduğunu düşünün. Bu sistem, bir kişinin fotoğrafına bakarak "şüpheli" olup olmadığına karar veriyor ve 0 ile 1 arasında bir "şüphelilik skoru" üretiyor. Skor 1'e ne kadar yakınsa, sistem o kişiyi o kadar şüpheli buluyor.

Bizim görevimiz, bir **karar eşiği (threshold)** belirlemek. Örneğin, "skoru 0.70'in üzerinde olan herkesi şüpheli kabul et" diyebiliriz.

*   **Çok Katı Bir Kural (Yüksek Eşik, örn: 0.95):** Bu durumda sadece sistemin çok emin olduğu kişileri "şüpheli" olarak işaretleriz. Sonuç? Neredeyse hiç masum insanı yanlışlıkla etiketlemeyiz (**düşük Sahte Pozitif Oranı**), ama belki de gerçekten şüpheli olan ama skoru 0.94'te kalmış birini gözden kaçırabiliriz (**düşük Gerçek Pozitif Oranı**).
*   **Gevşek Bir Kural (Düşük Eşik, örn: 0.20):** Bu durumda en ufak bir şüphede bile kişiyi "şüpheli" sayarız. Sonuç? Neredeyse tüm gerçek şüphelileri yakalarız (**yüksek Gerçek Pozitif Oranı**), ama bu süreçte bir sürü masum insanı da boş yere rahatsız etmiş oluruz (**yüksek Sahte Pozitif Oranı**).

Gördüğünüz gibi, burada bir ödünleşim (trade-off) var. ROC eğrisi, bu karar eşiğini en katıdan en gevşeye kadar tüm olası seviyeler için denediğimizde, "doğru tespit oranımız" ile "yanlış alarm oranımız" arasındaki bu ödünleşimi görselleştiren bir haritadır.

### ROC Eğrisi Nasıl Çizilir ve Yorumlanır?

Peki bu haritayı teknik olarak nasıl çizeriz? Gençler, süreç oldukça mantıklıdır. Modelimizin test verileri için ürettiği olasılık skorlarını alırız ve eşik değerini 1'den 0'a doğru sistematik olarak kaydırırız. Her bir eşik değeri için iki temel oranı hesaplarız:

1.  **Gerçek Pozitif Oranı (True Positive Rate - TPR):** Bu, bizim **Duyarlılık (Recall)** dediğimiz metriğin ta kendisidir. Gerçekte pozitif olan vakaların ne kadarını doğru bir şekilde yakalayabildiğimizi ölçer. Formülü: `TPR = TP / (TP + FN)`
2.  **Sahte Pozitif Oranı (False Positive Rate - FPR):** Bu ise "yanlış alarm oranımızdır". Gerçekte negatif olan vakaların ne kadarını hatalı bir şekilde pozitif olarak etiketlediğimizi gösterir. Formülü: `FPR = FP / (FP + TN)`

Grafiği oluşturma adımları şöyledir:
*   **Başlangıç:** Eşiği 1.0'a ayarlarız. Modelimiz çok "çekingendir" ve hiçbir şeye pozitif demez. Bu yüzden hem TPR hem de FPR sıfırdır. Bu, grafiğimizin sol alt köşesindeki **(0, 0)** noktasıdır.
*   **Ara Adımlar:** Eşiği yavaş yavaş düşürürüz. Eşik düştükçe model daha "cesur" hale gelir ve daha fazla örneğe pozitif demeye başlar. Bu süreçte hem TPR hem de FPR artar. Her eşik değeri için yeni bir (FPR, TPR) çifti hesaplar ve grafiğe bir nokta koyarız.
*   **Bitiş:** Eşiği 0.0'a getirdiğimizde, modelimiz her şeye pozitif der. Bu durumda hem tüm pozitifleri (TPR=1) hem de tüm negatifleri (FPR=1) "yakalamış" oluruz. Bu da grafiğimizin sağ üst köşesindeki **(1, 1)** noktasıdır.

Bu noktaları birleştirdiğimizde ortaya çıkan eğri, ROC eğrisidir.

![Örnek ROC Eğrileri](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ROC_curves.svg/440px-ROC_curves.svg.png)

*   **Grafik:** Yukarıdaki grafikte, x-ekseni **Sahte Pozitif Oranını (FPR)**, y-ekseni ise **Gerçek Pozitif Oranını (TPR)** temsil eder.
*   **Köşegen Çizgi (Rastgele Model):** (0,0)'dan (1,1)'e uzanan kesikli çizgi, yazı tura atmakla eşdeğer, yani tamamen rastgele tahmin yapan bir modeli simgeler. Bu modelin AUC değeri 0.5'tir.
*   **İdeal Nokta:** Sol üst köşe (0,1) noktası, mükemmel sınıflandırıcıyı temsil eder: Hiç yanlış alarm vermeden (%0 FPR) tüm gerçek pozitifleri yakalar (%100 TPR).
*   **Model Performansı:** Bir modelin eğrisi bu sol üst köşeye ne kadar yakınsa, performansı o kadar iyidir. Grafikteki yeşil eğri, mavi eğriden daha iyi bir performansa sahiptir.

### Eğrinin Altında Kalan Alan (Area Under the Curve - AUC)

Peki bu eğri bize ne anlatır? İdeal bir model, TPR'si 1 iken FPR'sinin 0 olduğu bir modeldir. Yani eğrinin grafiğin **sol üst köşesine** ne kadar yakınsa, modelin ayırt etme gücü o kadar iyidir.

Bu genel performansı tek bir sayı ile özetlemek için **Eğrinin Altında Kalan Alan (Area Under the Curve - AUC)** değerini hesaplarız.
*   **AUC = 1:** Mükemmel bir sınıflandırıcı. Pozitif ve negatif sınıfları hatasız bir şekilde ayırabiliyor.
*   **AUC = 0.5:** Rastgele tahmin yapan bir model. Yazı tura atmaktan farksızdır ve grafikte (0,0)'dan (1,1)'e uzanan köşegen çizgiyle temsil edilir.
*   **AUC < 0.5:** Rastgele tahminden daha kötü bir model.

AUC, modelin farklı eşik değerleri genelindeki ayırt etme gücünü gösteren, özellikle dengesiz veri setlerinde doğruluğa göre çok daha güvenilir ve yaygın olarak kullanılan bir metriktir.

## Pratik Uygulamalar

Bu metrikleri hesaplamak için hem görsel arayüzlü araçlar hem de programlama kütüphaneleri yaygın olarak kullanılır.

### Weka ile Değerlendirme

**Weka**, kod yazmadan makine öğrenmesi modelleri oluşturup değerlendirmenizi sağlayan popüler bir görsel araçtır.

1.  **Veri Setini Yükleme:** Weka "Explorer" arayüzünde, "Preprocess" sekmesinden `iris.arff` gibi hazır bir veri setini yükleyin.
2.  **Sınıflandırıcı Seçimi:** "Classify" sekmesinde, "Choose" butonu ile `trees` altından `J48` (bir karar ağacı algoritması) seçin.
3.  **Değerlendirme:** Test seçeneği olarak "Cross-validation" (Çapraz Doğrulama) kullanarak "Start" butonuna basın.
4.  **Sonuçları Yorumlama:** "Classifier output" panelinde şu sonuçları göreceksiniz:
    *   **`Correctly Classified Instances`**: **Doğruluk (Accuracy)**.
    *   **`Detailed Accuracy By Class`** tablosu: Her sınıf için `TP Rate` (**Recall**), `Precision`, `F-Measure` (**F1-Skoru**) ve `ROC Area` (**AUC**) değerlerini içerir.
    *   **`Confusion Matrix`**: Panelin en altında, öğrendiğimiz **Karışıklık Matrisi**'ni bulabilirsiniz.

Örnek bir Weka Karışıklık Matrisi çıktısı:
```
=== Confusion Matrix ===

  a  b  c   <-- classified as
 50  0  0 |  a = Iris-setosa
  0 47  3 |  b = Iris-versicolor
  0  1 49 |  c = Iris-virginica
```
Bu matris, `Iris-versicolor` sınıfından 3 örneğin hatalı bir şekilde `Iris-virginica` olarak sınıflandırıldığını açıkça gösterir.

### Python (Scikit-learn) ile Değerlendirme

Python'da makine öğrenmesi için en yaygın kütüphane olan **Scikit-learn**, tüm bu metrikleri hesaplamak için hazır fonksiyonlar sunar. Aşağıdaki örnek, bir modelin performansını nasıl değerlendireceğinizi gösterir. Bu kodu doğrudan bir Google Colab not defterinde çalıştırabilirsiniz.
Python'da makine öğrenmesi için en yaygın kütüphane olan **Scikit-learn**, bu metrikleri hesaplamak için bize son derece pratik fonksiyonlar sunar. Şimdi, öğrendiğimiz teorik bilgileri bir örnek üzerinde nasıl uygulayacağımızı görelim.

Aşağıdaki kod, sentetik bir veri seti oluşturur, bu veri üzerinde basit bir Lojistik Regresyon modeli eğitir ve ardından performansını, az önce öğrendiğimiz metrikler ve görsellerle kapsamlı bir şekilde analiz eder. Bu kodu doğrudan bir Google Colab not defterinde çalıştırarak sonuçları kendiniz de gözlemleyebilirsiniz.

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Veri Seti Oluşturma
# Sınıfları dengesiz bir veri seti oluşturalım.
# Örneklerin %90'ı 0. sınıfa, %10'u ise 1. sınıfa ait olacak.
X, y = make_classification(
    n_samples=1000,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    weights=[0.9, 0.1],
    flip_y=0,
    random_state=42
)

# 2. Veriyi Eğitim ve Test Olarak Ayırma
# stratify=y parametresi, eğitim ve test setlerindeki sınıf oranlarının
# orijinal veri setindekiyle aynı kalmasını sağlar. Bu, dengesiz veri setlerinde önemlidir.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 3. Model Eğitimi
model = LogisticRegression()
model.fit(X_train, y_train)

# 4. Tahmin Yapma
# predict() -> Nihai sınıf etiketini (0 veya 1) tahmin eder.
y_pred = model.predict(X_test)
# predict_proba() -> Her sınıf için olasılıkları verir. ROC eğrisi için gereklidir.
# [:, 1] ile sadece pozitif sınıfın (sınıf 1) olasılıklarını alıyoruz.
y_pred_proba = model.predict_proba(X_test)[:, 1]

# 5. Performans Metriklerini Raporlama
print("--- Sınıflandırma Raporu ---")
# classification_report, temel metrikleri düzenli bir formatta sunar.
print(classification_report(y_test, y_pred, target_names=['Negatif Sınıf (0)', 'Pozitif Sınıf (1)']))
print("-" * 30)

# 6. Karışıklık Matrisini Görselleştirme
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Tahmin Negatif', 'Tahmin Pozitif'],
            yticklabels=['Gerçek Negatif', 'Gerçek Pozitif'])
plt.title('Karışıklık Matrisi')
plt.show()

# 7. ROC Eğrisini Çizme
# roc_curve fonksiyonu, farklı eşik değerleri için FPR ve TPR'yi hesaplar.
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
# auc fonksiyonu, bu eğrinin altında kalan alanı hesaplar.
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Eğrisi (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Rastgele Tahmin')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Sahte Pozitif Oranı (FPR)')
plt.ylabel('Gerçek Pozitif Oranı (TPR)')
plt.title('Alıcı İşletim Karakteristiği (ROC) Eğrisi')
plt.legend(loc="lower right")
plt.show()
```



Gençler, bu kod parçasının ne yaptığını daha iyi anlayalım:

1.  **Veri Seti Oluşturma:** Gerçek dünya problemlerini daha iyi yansıtması için `make_classification` ile yapay bir veri seti oluşturduk. `weights=[0.9, 0.1]` parametresiyle, sınıflardan birinin diğerinden çok daha fazla örneğe sahip olduğu **dengesiz (imbalanced)** bir durum yarattık. Bu, `Accuracy` metriğinin neden tek başına yeterli olmayabileceğini göstermek için önemlidir.
2.  **Veriyi Ayırma:** Modelimizi eğitmek ve test etmek için veriyi ikiye ayırdık. `stratify=y` parametresi, bu dengesiz sınıf dağılımının hem eğitim hem de test setlerinde korunmasını garanti eder. Böylece modelimizi adil bir şekilde değerlendirebiliriz.
3.  **Model Eğitimi:** Basit ama güçlü bir sınıflandırma algoritması olan `LogisticRegression` modelini eğitim verileriyle (`X_train`, `y_train`) eğittik.
4.  **Tahmin Yapma:** Eğitilen modelimizi daha önce hiç görmediği test verileri (`X_test`) üzerinde çalıştırdık. Burada iki tür tahmin yaptık: `predict()` ile modelin kesin kararını (0 veya 1) ve `predict_proba()` ile modelin bir örneğin pozitif sınıfa ait olma olasılığını aldık. Bu olasılık değeri, ROC eğrisini çizmek için kritik öneme sahiptir.
5.  **Sınıflandırma Raporu:** `classification_report` fonksiyonu, her sınıf için Kesinlik (Precision), Duyarlılık (Recall) ve F1-Skoru değerlerini tek bir tabloda özetleyerek bize hızlı bir genel bakış sunar.
6.  **Karışıklık Matrisi:** Teoride gördüğümüz karışıklık matrisini `seaborn` kütüphanesiyle görselleştirdik. Bu ısı haritası, modelin ne tür hatalar yaptığını (FP veya FN) bir bakışta anlamamızı sağlar.
7.  **ROC Eğrisi ve AUC:** Modelin pozitif sınıf için ürettiği olasılıkları (`y_pred_proba`) kullanarak, farklı karar eşikleri için Sahte Pozitif Oranı (FPR) ve Gerçek Pozitif Oranı (TPR) değerlerini hesapladık. Bu noktaları birleştirerek ROC eğrisini çizdik. Eğrinin altında kalan alan (AUC), modelin genel ayırt etme gücünün sayısal bir ölçüsüdür. AUC değeri 1'e ne kadar yakınsa, model o kadar iyidir.

## Son Sözler

Gençler, unutmayın ki tek bir "en iyi" performans ölçütü yoktur; probleme ve hedefe "en uygun" ölçüt vardır. Bir modelin başarısını değerlendirirken, problemin bağlamını göz önünde bulundurarak birden fazla metriği birlikte analiz etmek esastır.

*   Bir hastalığın teşhisinde, hiçbir vakayı atlamamak öncelikli olduğu için **Duyarlılık (Recall)** kritik olabilir.
*   Bir spam filtresinde, önemli bir e-postayı yanlışlıkla engellememek için **Kesinlik (Precision)** daha önemli olabilir.
*   Sınıfların dengesiz olduğu durumlarda **Doğruluk (Accuracy)** yerine **F1-Skoru** veya **AUC** gibi metriklere odaklanmak daha sağlıklı sonuçlar verir.

Bu metrikler, geliştirdiğiniz modelleri anlama ve iyileştirme sürecinizde size yol gösterecek temel araçlardır.

